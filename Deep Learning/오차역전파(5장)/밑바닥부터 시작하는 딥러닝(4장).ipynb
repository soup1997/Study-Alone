{"cells":[{"cell_type":"markdown","metadata":{"id":"7k34p-MTOWTT"},"source":["# 신경망 학습\n","* 학습: 훈련 데이터로부터 가중지 매개변수의 최적값을 자동으로 획득하는 것\n","* 손실함수: 신경망이 학습할 수 있도록 해주는 지표   \n","\n","## 목표\n","* 손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는것"]},{"cell_type":"markdown","metadata":{"id":"iiyktFDgRFFl"},"source":["# MSE(Mean Squred Error)\n","> $E = \\frac{1}{2}\\Sigma_k(y_k-t_k)^2$\n","\n","평균 제곱 오차를 기준으로 이에 대한 값이 작을 수록 정확한 모델    \n","$y_k$가 특정 레이블에 대한 확률값을 저장한 배열 그리고, $t_k$가 원-핫 인코딩된 정답값을 가지고 있는 배열이라 할때 MSE는 0~1사이의 값을 가진다."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"NTH4GBSSOxIg","executionInfo":{"status":"ok","timestamp":1675412655057,"user_tz":-540,"elapsed":4,"user":{"displayName":"조현습","userId":"00227774474014694265"}}},"outputs":[],"source":["import numpy as np\n","\n","def mean_squared_error(y, t):\n","  return 0.5 * np.sum((y-t)**2)"]},{"cell_type":"markdown","metadata":{"id":"EIjMccmPR_Qw"},"source":["# CEE(Cross Entropy Error)\n","> $E = -\\Sigma t_k \\log{y_k}$\n","\n","마찬가지로 $y_k$는 신경망의 출력, $t_k$는 정답 레이블   \n","교차 엔트로피 오차를 기준으로 이에 대한 값이 작을 수록 정확한 모델"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VDTgv5w3R1KR","executionInfo":{"status":"ok","timestamp":1675412655058,"user_tz":-540,"elapsed":4,"user":{"displayName":"조현습","userId":"00227774474014694265"}}},"outputs":[],"source":["def cross_entropy_error(y, t):\n","  delta = 1e-7\n","  return -np.sum(t * np.log(y + delta)) # np.log(0)일 경우 -inf로 발산, 아주 작은 값을 더하여 예외처리"]},{"cell_type":"markdown","metadata":{"id":"64lseyyLTgAu"},"source":["# 미니배치 학습(데이터 1개가 아닌, 훈련 데이터의 묶음을 고려한 손실함수 수식)\n","> $E = -\\frac{1}{N}\\Sigma_n \\Sigma_k t_{nk} \\log{y_{nk}}$\n","\n","* 데이터가 총 N개라면 $t_{nk}$는 $n$번째 데이터의 $k$번째 값을 의미함\n","* $y_{nk}$는 신경망의 출력, $t_{nk}$는 정답 레이블\n","* 마지막에 $N$으로 나눔으로써 정규화\n","\n","## Mini-batch(미니배치)\n","전체 훈련 데이터의 대표로서 무작위로 선택한 작은 덩어리(미니배치)\n","* MNIST 데이터셋의 경우 훈련 데이터가 60000개\n","* 모든 데이터를 대상으로 손실 함수의 합을 구하기에는 오버헤드가 크다.\n","* 따라서, 데이터 일부를 추려 전체의 '근사치'로 이용\n","* 예를들어, 60000장의 훈련 데이터 중에서 반복적으로 100장을 무작위로 뽑아 그 100장만을 사용하여 학습(미니배치 학습) "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20443,"status":"ok","timestamp":1675412675497,"user":{"displayName":"조현습","userId":"00227774474014694265"},"user_tz":-540},"id":"5hiLq5p1TO18","outputId":"85610a33-400d-4434-ff67-9c7070486195"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","x_trains shape: (60000, 784)\n","x_tests shape: (10000, 784)\n","t_trains shape: (60000, 10)\n","t_tests shape: (10000, 10)\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/Colab_Notebooks')\n","\n","from sample.dataset.mnist import load_mnist\n","\n","def get_data():\n","  (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True) # 정답 레이블 정규화, one-hot-encoding 적용\n","\n","  print('x_train''s shape:', x_train.shape)\n","  print('x_test''s shape:', x_test.shape)\n","  print('t_train''s shape:', t_train.shape)\n","  print('t_test''s shape:', t_test.shape)\n","\n","  return x_train, t_train, x_test, t_test\n","\n","x_train, t_train, x_test, t_test = get_data()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1675412675498,"user":{"displayName":"조현습","userId":"00227774474014694265"},"user_tz":-540},"id":"_X9NM_rBb-wo","outputId":"d05f9765-f2df-4b08-8448-0b770b014072"},"outputs":[{"output_type":"stream","name":"stdout","text":["batch_mask:  [40527  4095 12984 25320  1922 52723 32016  7170  3476 58468]\n"]}],"source":["import numpy as np\n","# minibatch 학습을 위한 무작위로 훈련 셋 10장 뽑기\n","\n","'''\n","np.random.choice(a, size=None, replace=True, p=None)\n","  Generates a random sample from a given 1-D array\n","'''\n","train_size = x_train.shape[0]\n","batch_size = 10\n","batch_mask = np.random.choice(train_size, batch_size)\n","print('batch_mask: ', batch_mask)\n","\n","x_batch = x_train[batch_mask]\n","t_batch = t_train[batch_mask]"]},{"cell_type":"markdown","metadata":{"id":"YUc-zY4TgFTe"},"source":["# (배치용) 교차 엔트로피 오차 구현하기\n","데이터가 하나로 통째로 묶어있는 경우와, 데이터가 배치로 묶여 입력될 경우 모두를 처리할 수 있는 함수\n","\n","* y가 1차원이라면, reshape로 데이터의 형상을 바꿈\n","* 배치의 크기로 나눠 정규화 하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bJaqUM9ygPgv","executionInfo":{"status":"ok","timestamp":1675412675498,"user_tz":-540,"elapsed":4,"user":{"displayName":"조현습","userId":"00227774474014694265"}}},"outputs":[],"source":["def cross_entropy_error(y, t):\n","  if y.ndim == 1:\n","    t = t.reshape(1, t.size)\n","    y = y.reshape(1, y.size)\n","\n","  batch_size = y.shape[0]\n","  return -np.sum(t * np.log(y + 1e-7)) / batch_size"]},{"cell_type":"markdown","metadata":{"id":"_1nIuXOuicyi"},"source":["# 왜 손실함수를 정의하는가?\n","* 신경망 학습에서는 최적의 매개변수(weight, bias)를 탐색할 때 손실함수의 값을 가능한 한 작게하는 매개변수 값을 찾는다.\n","* 즉, 손실함수의 최솟값 $\\rightarrow$ 미분해서 0이되는 지점\n","* 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복\n","* 미분값이 음수면 그 자중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있음(2차원 비선형 곡선을 생각하면 됨)\n","* 미분 값이 0이면 가중치 매개변수를 어느쪽으로 움직여도 손실 함수의 값은 달라지지 않는다 $\\rightarrow$따라서 그 가중치 매개변수의 갱신을 멈춤   \n","\n","`\n","결론적으로, 신경망을 학습할 때 정확도를 지표로 삼아서는 안된다. \\\n","정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다. \\\n","따라서 손실함수를 지표로 삼아야 한다.\n","`\n","* 이는 unit step function을 활성화 함수로 사용하지 않는 이유와 마찬가지이다.\n","* 대부분의 장소에서 미분값이 0이기 때문에, 계단 함수를 이용하면 손실함수를 지표로 삼는게 아무 의미가 없게 된다.(discrete system의 대표적인 함수이기 때문)\n","* 따라서 sigmoid function $f(x)=\\frac{1}{1+\\exp^{-x}}$을 이용하여 연속적인 그래프를 사용하여 신경망을 올바르게 학습시킴"]},{"cell_type":"markdown","metadata":{"id":"uJQ3bgzapOod"},"source":["# 기울기(Gradient)\n","> **Example**   \n","> $f(x_0, x_1)=x_0^2+x_1^2$   \n","> $(\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1})$ 처럼 모든 변수의 편미분을 **벡터**로 정리한 것을 Gradient(기울기)라고 칭함.   \n","> 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향\n"]},{"cell_type":"markdown","metadata":{"id":"8lOyJINmrBJd"},"source":["# 경사 하강법(Gradient Descent)\n","* $\\eta$는 갱신하는 양을 나타냄, 신경망 학습에서 학습률(Learning rate)라고 칭함\n","* 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률\n","\n","> $x_0=x_0-\\eta\\frac{\\partial f}{\\partial x_0}$   \n","> $x_1=x_1-\\eta\\frac{\\partial f}{\\partial x_1}$ \n","\n","* 특히나 학습률 값은 `0.01`이나 `0.001` 등 미리 특정 값으로 정해두어야 함.\n","* 일반적으로 이 값이 너무 크거나 작으면 최솟값으로 찾아갈 수 없음\n","* 신경망 학습에서 보통 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인\n","* 학습률이 너무 크면 큰 값으로 발산, 학습률이 너무 작으면 거의 갱신되지 않은 채 반복 종료"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZEEr7YpOghfP","executionInfo":{"status":"ok","timestamp":1675412675498,"user_tz":-540,"elapsed":4,"user":{"displayName":"조현습","userId":"00227774474014694265"}}},"outputs":[],"source":["def numerical_gradient(f, x):\n","  h = 1e-4 # 0.0001\n","  grad = np.zeros_like(x)\n","\n","  for idx in range(x.size):\n","    tmp_val = x[idx]\n","\n","    # f(x+h) 계산\n","    x[idx] = tmp_val+h\n","    fxh1 = f(x)\n","\n","    # f(x-h) 계산\n","    x[idx] = tmp_val - h\n","    fxh2 = f(x)\n","\n","    grad[idx] = (fxh1 - fxh2) / (2 * h)\n","    x[idx] = tmp_val\n","\n","  return grad\n","\n","'''\n","gradient_descent\n","<param:\n","  f: 최적화 하려는 함수\n","  init_x: 초깃값\n","  lr: learning rate\n","  step_num: 경사법에 따른 반복 횟수\n",">\n","<return:\n","  x: 최소지점의 x값\n",">\n","'''\n","def gradient_descent(f, init_x, lr=0.01, step_num=100):\n","  x = init_x\n","\n","  for i in range(step_num):\n","    grad = numerical_gradient(f, x)\n","    x -= lr * grad\n","  \n","  return x"]},{"cell_type":"markdown","metadata":{"id":"6-GTqwJFwnN3"},"source":["# 학습 알고리즘\n","전제: 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.   \n","> * **1단계(Mini Batch)**: 훈련 데이터 중 일부를 무작위로 가져옴. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표\n","> * **2단계(Calculate Gradient)**: 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시  \n","> * **3단계(Update Prameter)**: 가중치 매개변수를 기울기 방향으로 아주 조금 갱신   \n","> * **4단계(Repeatition)**: 1~3단계를 반복   \n","\n","`데이터를 미니배치로 무작위로 선정하기 때문에 확률적 경사 하강법(stocahstic gradient descent; SGD)라고 칭한다.`"]},{"cell_type":"markdown","metadata":{"id":"Ys9u43bW6d6F"},"source":["# 미니배치 학습 구현"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"GEZXItZLvQg1","executionInfo":{"status":"ok","timestamp":1675412676281,"user_tz":-540,"elapsed":787,"user":{"displayName":"조현습","userId":"00227774474014694265"}}},"outputs":[],"source":["# 2층 신경망 클래스 구현하기\n","import matplotlib.pyplot as plt\n","from sample.common.functions import *\n","from sample.common.gradient import numerical_gradient\n","\n","class TwoLayerNet:\n","  '''\n","  input_size: 입력층의 뉴런 수 \n","  hidden_size: 은닉층의 뉴런 수\n","  output_size: 출력층의 뉴런 수\n","  '''\n","  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","    self.params = {}\n","    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","    self.params['b1'] = np.zeros(hidden_size)\n","    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","    self.params['b2'] = np.zeros(output_size)\n","  \n","  '''\n","  predict:\n","  가중치를 토대로 데이터셋과 연산, \n","  활성화 함수 적용, \n","  예측값 출력\n","  '''\n","  def predict(self, x):\n","    W1, W2 = self.params['W1'], self.params['W2']\n","    b1, b2 = self.params['b1'], self.params['b2']\n","\n","    a1 = np.dot(x, W1) + b1\n","    z1 = sigmoid(a1)\n","    a2 = np.dot(z1, W2) + b2\n","    y = softmax(a2)\n","\n","    return y\n","\n","  # x: 입력 데이터, t: 정답 레이블\n","  def loss(self, x, t):\n","    y = self.predict(x)\n","    return cross_entropy_error(y, t)\n","\n","  # 정확도를 출력하는 함수\n","  def accuracy(self, x, t):\n","    y = self.predict(x)\n","    y = np.argmax(y, axis=1)\n","    t = np.argmax(t, axis=1)\n","\n","    accuracy = np.sum(y == t) / float(x.shape[0]) # 입력된 데이터에 대하여 일치하는 확률을 구함\n","\n","    return accuracy\n","\n","  # 가중치의 기울기를 구하는 함수(편미분)\n","  # x: 입력 데이터, t: 정답 레이블\n","  def numerical_gradient(self, x, t):\n","    loss_W = lambda W: self.loss(x, t)\n","\n","    grads = {}\n","    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","    return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"6WYd4Ya92Ti1","outputId":"37a94ffb-6cc3-498d-f829-d770da5e2134"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_trains shape: (60000, 784)\n","x_tests shape: (10000, 784)\n","t_trains shape: (60000, 10)\n","t_tests shape: (10000, 10)\n"]}],"source":["x_train, t_train, x_test, t_test = get_data()\n","train_loss_list = []\n","\n","# Hyper Parmater\n","iters_num = 1000 # 가중치를 SGD를 적용하기 위한 반복 횟수\n","train_size = x_train.shape[0]\n","batch_size = 100 # 미니배치 크기\n","learning_rate = 0.1\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","for i in range(iters_num):\n","  # 미니배치 획득\n","  batch_mask = np.random.choice(train_size, batch_size)\n","  x_batch = x_train[batch_mask]\n","  t_batch = t_train[batch_mask]\n","\n","  # 기울기 계산\n","  grad = network.numerical_gradient(x_batch, t_batch)\n","  # grad = network.gradient(x_batch, t_batch) # 성능 개선판\n","\n","  # 매개변수 갱신\n","  for key in ('W1', 'b1', 'W2', 'b2'):\n","    network.params[key] -= learning_rate * grad[key]\n","\n","  # 학습 결과 기록\n","  loss = network.loss(x_batch, t_batch)\n","  train_loss_list.append(loss)\n","\n","iteration = range(iters_num)\n","plt.plot(iteration, train_loss_list)\n","plt.xlabel('iteration')\n","plt.ylabel('loss')"]},{"cell_type":"markdown","metadata":{"id":"WJZNJ0KqGYI9"},"source":["# 시험 데이터로 평가하기\n","* 오버피팅(Overfitting): 훈련 데이터에 포함된 데이터만 제대로 구분하고, 그렇지 않는 데이터는 식별할수 없는것. 즉, 범용적인 능력을 가지지 못하는것\n","\n","## 1 Epoch 마다 훈련 데이터와 시험 데이터에 대한 정확도 기록\n","* Epoch: 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. \\\n","훈련 데이터 10000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면\\\n","모든 훈련 데이터를 '소진'하기 된다. 이 경우 100회가 1에폭이 된다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CrVKaUwG6-Ts"},"outputs":[],"source":["# 2층 신경망 클래스 구현하기\n","import matplotlib.pyplot as plt\n","from sample.common.functions import *\n","from sample.common.gradient import numerical_gradient\n","\n","class TwoLayerNet:\n","  '''\n","  input_size: 입력층의 뉴런 수 \n","  hidden_size: 은닉층의 뉴런 수\n","  output_size: 출력층의 뉴런 수\n","  '''\n","  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","    self.params = {}\n","    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","    self.params['b1'] = np.zeros(hidden_size)\n","    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","    self.params['b2'] = np.zeros(output_size)\n","  \n","  '''\n","  predict:\n","  가중치를 토대로 데이터셋과 연산, \n","  활성화 함수 적용, \n","  예측값 출력\n","  '''\n","  def predict(self, x):\n","    W1, W2 = self.params['W1'], self.params['W2']\n","    b1, b2 = self.params['b1'], self.params['b2']\n","\n","    a1 = np.dot(x, W1) + b1\n","    z1 = sigmoid(a1)\n","    a2 = np.dot(z1, W2) + b2\n","    y = softmax(a2)\n","\n","    return y\n","\n","  # x: 입력 데이터, t: 정답 레이블\n","  def loss(self, x, t):\n","    y = self.predict(x)\n","    return cross_entropy_error(y, t)\n","\n","  # 정확도를 출력하는 함수\n","  def accuracy(self, x, t):\n","    y = self.predict(x)\n","    y = np.argmax(y, axis=1)\n","    t = np.argmax(t, axis=1)\n","\n","    accuracy = np.sum(y == t) / float(x.shape[0]) # 입력된 데이터에 대하여 일치하는 확률을 구함\n","\n","    return accuracy\n","\n","  # 가중치의 기울기를 구하는 함수(편미분)\n","  # x: 입력 데이터, t: 정답 레이블\n","  def numerical_gradient(self, x, t):\n","    loss_W = lambda W: self.loss(x, t)\n","\n","    grads = {}\n","    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","    return grads\n","\n","\n","x_train, t_train, x_test, t_test = get_data()\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 1에폭당 반복 수\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","# Hyper Parmater\n","iters_num = 10000 # 가중치를 SGD를 적용하기 위한 반복 횟수\n","train_size = x_train.shape[0]\n","batch_size = 100 # 미니배치 크기\n","learning_rate = 0.1\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","for i in range(iters_num):\n","  # 미니배치 획득\n","  batch_mask = np.random.choice(train_size, batch_size)\n","  x_batch = x_train[batch_mask]\n","  t_batch = t_train[batch_mask]\n","\n","  # 기울기 계산\n","  grad = network.numerical_gradient(x_batch, t_batch)\n","  # grad = network.gradient(x_batch, t_batch) # 성능 개선판\n","\n","  # 매개변수 갱신\n","  for key in ('W1', 'b1', 'W2', 'b2'):\n","    network.params[key] -= learning_rate * grad[key]\n","\n","  # 학습 결과 기록\n","  loss = network.loss(x_batch, t_batch)\n","  train_loss_list.append(loss)\n","\n","  if i % iter_per_epoch == 0:\n","    train_acc = network.accuracy(x_train, t_train)\n","    test_acc = network.accuracy(x_test, t_test)\n","    train_acc_list.append(train_acc)\n","    test_acc.append(test_acc)\n","    print('train acc, test_acc |' + str(train_acc) + \", \" + str(test_acc))"]},{"cell_type":"code","source":["#### plot loss, accuracy ####\n","iteration = list(range(iters_num))\n","epoch = list(range(0, iters_num, int(iter_per_epoch)))\n","\n","plt.figure(figsize=(20,10))\n","plt.subplot(1, 2, 1)\n","plt.plot(iteration, train_loss_list)\n","plt.xlabel('iteration')\n","plt.ylabel('loss')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epoch, train_acc_list, label='train accuracy')\n","plt.plot(epoch, test_acc_list, label='test accuracy')\n","plt.xlabel('epoch')\n","plt.ylabel('accuracy')\n","plt.legend(loc='upper right')\n","plt.grid(True)\n","\n","plt.show()"],"metadata":{"id":"AnQAtP6DcXeH"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["7k34p-MTOWTT"],"provenance":[],"authorship_tag":"ABX9TyP+nBpXKGeqeV680cq29Odf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}